\documentclass[runningheads]{llncs}

\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{booktabs} % For pretty tables
\usepackage{caption} % For caption spacing
\usepackage{subcaption} % For sub-figures
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[all]{nowidow}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usetikzlibrary{er,positioning,bayesnet}
\usepackage{multicol}
\usepackage{algpseudocode,algorithm,algorithmicx}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[inline]{enumitem} % Horizontal lists
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\card}[1]{\left\vert{#1}\right\vert}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\definecolor{blue}{HTML}{1F77B4}
\definecolor{orange}{HTML}{FF7F0E}
\definecolor{green}{HTML}{2CA02C}

\pgfplotsset{compat=1.14}

\parskip 5mm
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.10}
\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\textfraction}{0.1}
\setlength{\floatsep}{3pt plus 1pt minus 1pt}
\setlength{\textfloatsep}{5pt plus 1pt minus 1pt}
\setlength{\intextsep}{3pt plus 1pt minus 1pt}
\setlength{\abovecaptionskip}{2pt plus 1pt minus 1pt}

\begin{document}
%
\title{Interpretable XRL}

\author{Montenegro Meza Mauro \inst{1} \and
Rolando Menchaca-Méndez.\inst{1} \and
Ricardo Menchaca Méndez\inst{1}}

\institute{Network and Data Science Laboratory, Centro de Investigación en Computación, Mexico City, México}
%
\maketitle              
%
\begin{abstract}


\keywords{Reinforcement Learning \and XRL \and Interpretability}
\end{abstract}

\section{Introduction}

In recent years, many accurate decision support systems have been constructed as black boxes, that is, as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical (harder to deploy) and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability \cite{johansson2011trade}.

Besides the incremental set of Explainable Artificial Intelligence (XAI) literature, there is a lack of work in Reinforcement Learning (RL) field. Recent works propose various RL interpretation methods and are applied to multiple tasks, but, while the main objective of all algorithms is to make interpretable models, each method has special purposes and has unique sets of limitations and challenges. This makes harder to make a clear classification of methods. Due to the above problem of classification and generality of methods comes another important challenge: Metrics for comparing algorithms, since accuracy is not the only relevant parameter when comes to interpretability. In XAI field there are some proposed quantitative and qualitative metrics (ref 1) (ref 2) (ref 3) that could be used in XRL.

There are some papers that propose a classification on methods for interpretability in RL (ref1) (ref2) (ref3). There is a clear difference on the top level of any classification method: We modify the model to make it interpretable by itself or we use a surrogate to imitate their predictions?. This comes with a discussion about the ambiguity of interpretability meaning. Gilpin et al (ref) tried to explain differences between \textit{interpretability} (the answer for a why question) and \textit{explainability} (the ability to provide understandable explanations and process to the end user).

The main goal of this paper is to list and compare different methods and metrics used to interpret RL and compare frameworks that are based on making a model interpretable or build a surrogate that explains its outcomes. On the top of that there is a discussion about the importance of making XRL and XAI for society and research community. 

The rest of the paper is organized as follows: Section II provides a discussion about the relevance and need to make interpretable XAI and XRL for ethical issues and to improve algorithms. In Section III we develop the mathematical framework where RL is based: Markov Decision Processes and RL algorithms. Section IV illustrates the taxonomy existing in literature and metrics to evaluate their interpretability measures while making a clear distinction between interpretable architecture and explainable models. In Section V we provide relevant interpretable and explainable methods in literature and apply metrics to evaluate their performance. Finally, Section VI gives the conclusion of the paper.


\section{Relevance of Interpretability}

Before showing the classification addressed in literature regarding to Machine Learning models and his solutions framework categorization, we need to discuss about the concepts of knowledge and interpretability. To interpret means to give or provide the meaning or to explain and present in understandable terms some concepts. \footnote{www.merriam-webster.com} Currently, there is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. Besides this fact, some authors try to give some general concept. For Doshi et. al. \cite{Doshi-velez2017}  interpretability is the ability to explain or to present in understandable terms to a human. These definitions assume implicitly that the concepts expressed in the understandable terms composing an explanation are self-contained and do not need further explanations. \cite{Guidotti2018}

The aim of interpretability is to present knowledge embedded in the network in a way that a human can understand as stated before. But, although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it —What is it?— has rarely been answered directly. Davis et. al \cite{Cercone1987} argues that notion of knowledge representation can best understood in terms of five distinct roles. First, it works as a surrogate or substitute for the thing itself, and is used to enable an entity to determine consequences by thinking rather than acting. Second, it is a set of ontological commitments, that is, an answer to the question "In what terms should i think about the world?". Third, as a fragmentary theory of intelligent reasoning expressed in terms of:  the representation´s fundamental conception of intelligent reasoning, the set of inferences that representation sanctions and the ones that recommends. Fourth, knowledge representation is a medium for pragmatically efficient computation and, fifth, it is also a medium of human expression, that is, a language in which we say things about the world.

A Machine Learning model often is referred as a "black box" or "obscure" model, whose internals are either unknown to the observer or they are known but uninterpretable to humans. So, taking account of our earlier discussion, making this model interpretable is to present as a surrogate the knowledge in a way that is clear to a human.

To make an interpretable model, it is necessary to take into account some features mentioned in the state of the art \cite{Andrews1995} \cite{Doshi-velez2017} \cite{Freitas} and Guidotti et. al. \cite{Guidotti2018} remarks 3 of them:

\begin{enumerate}
    \item Interpretability: How much the model or its predictions are human understandable and how we can measure it. According to literature, we refer to this feature also with the name of comprehensibility.
    \item Accuracy: To which extent the model accurately predicts new data.This feature can be measured using evaluations like the accuracy score or the F1-score \cite{tan2006introduction}. Most papers in literature aim to produce comprehensive models maintaining a high accuracy score. 
    \item Fidelity: The model is able to accurate imitate the black-box predictions?. This fidelity feature captures how much of the original model this interpretation have. This could be measured as the accuracy but in terms of the outcome of the black-box model.
\end{enumerate}

Besides these features are the main goal of making a model comprehensive, we need to acknowledge some ethical issues. According to \cite{Andrews1995} \cite{Doshi-velez2017} fairness and privacy are important aspects when we are dealing with models that apply directly to people.

The first principle requires that the model guarantees the protection of groups against (direct or indirect) discrimination \cite{romei2014multidisciplinary}; while the second one requires that the model does not reveal sensitive information about people \cite{aldeen2015comprehensive}. The level of interpretability of a model together with the standards of privacy and non-discrimination that are guaranteed may impact on how much human users trust that model.\cite{Guidotti2018}

%Explain social and academic impact on design interpretable models for DRL. First, consider ethical and regulation issues (DGPR) and then, show improvements in algorithms making them interpretable. Also, i like to add some discussion about interpretable vs explainable methods and maybe compare(improvement in quality results or interpretable metrics) what approach brings more advantages.

\section{Background}

Reinforcement Learning (RL) is an emerging field of Machine Learning (ML) that gives solutions to problems where the main objective is wrapped in a Markov Decision Process (MDP). It involves the study of agents that interact with a dynamic  environment through actions, and the goal is to optimize a cumulative reward over time.

MDP is a mathematical framework used to solve control decision processes. All the states should satisfy the Markov Property: The resultant state of any action does not depend on the history, instead it only depends on the current state.

We describe this process as follows: A state $s \in S$ for the agent in the environment, it can choose any valid action $a \in A(s)$. Taking an action changes the agent state from $s$ to $s'$ and following a transition $T$, this process comes with a reward $R(s,a)$. Mathematically an MDP is a tuple of $(S,A,T,R,\gamma)$. Future rewards are penalized by the factor $\gamma$. 

When the state is not fully observable by the agent, MDP becomes a Partially Observable Markov Decision Process (POMDP) defined by the tuple  $(S , A , T , R , \Omega , O , \gamma$) where $\Omega$ and $O$ are the set of all observations and the set of observation probabilities.

A policy $\pi$ can be defined as the strategy that the agent follows, a mapping between agent states and action at that state. A policy could be deterministic or non-deterministic. Under a a probabilistic format, this mapping is represented by Equation \ref{eq:1}:

\begin{equation}
\label{eq:1}
\pi(a|s) = Pr(a_t=a|s_t=s)    
\end{equation}


An optimal policy is the one that maximizes or minimizes the cumulative discounted reward described by the Equation \ref{eq:2}. This optimization can be formulated using the Bellman Equation \ref{eq:3} which express the optimal policy for a given state $s$.

\begin{equation}
\label{eq:2}
    V(s)=E[\sum_{t=1}^{T}\gamma^{t-1}r]
\end{equation}

\begin{equation}
\label{eq:3}
    \pi^*(s)=argmax_a \sum_{s' \in S}T(s,a,s')V^{\pi ^*}(s')
\end{equation}


\section{Related Work}



\subsection{Global vs Local }
Explanation of the total model or focus on a specific decision.

\subsection{Transparent vs Post-Hoc }
Model inherently interpretable or after train the model provide explanations trough a proxy.

\section{Metrics}
Review of some metrics used to measure explainability, interpretability.

\subsection{Evaluating Explanations}
Take a subset of relevant work in order to compare algorithms in terms of interpretability.

\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{biblio}
%
\end{document}
